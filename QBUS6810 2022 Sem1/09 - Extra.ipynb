{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Notes\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "[Visualisation of Decision tree process](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)\n",
    "\n",
    "[Example of Non-Linear Data](https://www.jeremyjordan.me/decision-trees/)\n",
    "\n",
    "[Example of decision tree pruity](https://gdcoder.com/decision-tree-regressor-explained-in-depth/)\n",
    "\n",
    "### Cost Complexity Pruning\n",
    "\n",
    "https://www.isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_27/lecture_27_07.html\n",
    "\n",
    "https://sanchitamangale12.medium.com/decision-tree-pruning-cost-complexity-method-194666a5dd2f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Jelly Beans in a Jar\n",
    "Suppose I had a jar of Jelly Beans and I wanted you to guess the number of jelly beans in this jar.\n",
    "\n",
    "<center><img src=\"https://cdn.shopify.com/s/files/1/1009/8338/products/2015-10-12-18.42.jpg?v=1445378595\" width=\"250\"></center>\n",
    "\n",
    "Of course, it is very difficult to get the exact number of jelly beans. But, instead if I take an average of 1000 predictions, turns out this average because very close to the exact number.\n",
    "\n",
    "[TIL that 160 people were able to guess the number of jelly beans in a jar accurate to .1% when their answers were averaged. Out of 4510 beans, the average guess was 4514. The phenomena is known as wisdom of the crowd](https://old.reddit.com/r/todayilearned/comments/1ogik8/til_that_160_people_were_able_to_guess_the_number/#:~:text=InSign%20Up-,TIL%20that%20160%20people%20were%20able%20to%20guess%20the%20number,as%20wisdom%20of%20the%20crowd).\n",
    "\n",
    "This idea is known as the wisdom of the crowd-\n",
    "\n",
    "    The wisdom of the crowd is the collective opinion of a group of individuals rather than that of a single expert ... An explanation for this phenomenon is that there is idiosyncratic noise associated with each individual judgment, and taking the average over a large number of responses will go some way toward canceling the effect of this noise.\n",
    "    - Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Ensemble Models\n",
    "Ensembling is a very easy idea. Instead of building 1 model, why don't we build $M$ models and take the average of them. In particular, if $G_1(x), G_2(x),\\cdots G_M(x)$ are our $M$ models (importantly these models can all be different, one can be linear regression, KNN, trees, etc.). Then the ensemble prediction is simply the average of these models.\n",
    "\n",
    "$$\\mathcal{G}_M(x)=\\frac{1}{M}\\sum_{i=1}^MG_i(x)$$\n",
    "\n",
    "### Why Ensemble\n",
    "Take $X_i$ to be random variable, and independent and identically distributed. Than what we know is that if\n",
    "$$\\textrm{Var}(X_i)=\\sigma^2$$\n",
    "\n",
    "Then the variance of the average $\\bar{X}$\n",
    "$$\\textrm{Var}(\\bar{X})=\\textrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right)=\\frac{\\sigma^2}{n}$$\n",
    "\n",
    "In otherwords, the variance of the average decreases as we increase $n$, the sample size.\n",
    "\n",
    "How this relates to use is that we can think of the prediction of a given model (could be any model, doesn't need to be trees) to be a random variable. Then when we ensemble models together, we are effectively taking the average of all these random variables. As such, this will decrease the variance of our models.\n",
    "\n",
    "However, our models aren't independent, typically. So, if $X_i$ are identically distributed (dropping independent assumption), then\n",
    "$$\\textrm{Var}(\\bar{X})=\\rho\\sigma^2+\\frac{1-\\rho}{n}\\sigma^2$$\n",
    "where $\\rho$ is the correlation of $X_i$'s.\n",
    "\n",
    "So really when we are ensembling we want to do two things. \n",
    "1. Increase $M$, the number of models, \n",
    "2. Make the correlations $\\rho$ as small as we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to Ensemble\n",
    "1. Different algorithms\n",
    "2. Different training sets\n",
    "3. Bagging or Bootstrapping (Random Forests)\n",
    "4. Boosting (xgboost)\n",
    "\n",
    "### Bootstrapping\n",
    "Bootstrapping is a general techinique that can be used to estimate any statistical measure. \n",
    "\n",
    "The main idea is to create several subsets of data from training sample chosen randomly with replacement. Now, each collection of subset data is used to train their decision trees. As a result, we end up with an ensemble of different models.\n",
    "\n",
    "https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "X = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap = {}\n",
    "\n",
    "bootstrap[1] = [random.choice(X) for i in range(5)]\n",
    "bootstrap[2] = [random.choice(X) for i in range(5)]\n",
    "bootstrap[3] = [random.choice(X) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 1: [4, 2, 4, 5, 6] -> Train -> Model 1\n",
      "Bootstrap 2: [2, 1, 4, 3, 2] -> Train -> Model 2\n",
      "Bootstrap 3: [2, 6, 2, 2, 6] -> Train -> Model 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3 + 1):\n",
    "    print(f'Bootstrap {i}: {bootstrap[i]} -> Train -> Model {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are training the model on different bootstraps of the data, it will decorrelate the models. Recall\n",
    "\n",
    "$$\\textrm{Var}(\\bar{X})=\\rho\\sigma^2+\\frac{1-\\rho}{M}\\sigma^2$$\n",
    "\n",
    "Hence by driving down $\\rho$, the correlation, it leads to  less variance. But, bias slightly increases\n",
    "\n",
    "Of course, the more bootstrap samples you make the more models you will train. Hence $M$ increases and resulting in less variance, bias slightly increases\n",
    "\n",
    "**Jelly Beans**\n",
    "\n",
    "Going back to the jar of jelly beans, under this new knowledge, it is pretty simply why this works. We are taking a bunch of humans, we train then, and take an ensumble of them. Lucky for us, humans are already decorrelated with themselves. \n",
    "\n",
    "But this ensumble will then reduce the variance and bias of humans, hence giving an accurate prediction.\n",
    "\n",
    "### Why Ensemble Trees\n",
    "Thinking about the bias variance decomposition of Trees, trees have low bias but high variance. Making an ideal fit for bagging\n",
    "\n",
    "### Random Forest\n",
    "Note that bagging (or bootstrapping) only bootstraps the data that each individual trees are trained on. \n",
    "\n",
    "But, if we want to decorrelated further we can use a random forest. Where only a fraction of the features is considered at each split (drives down $\\rho$) decorellates models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [normalize, f, g, over, under, svc]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores(pipelien, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = under(over(X, y))\n",
    "svc(resampled_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
