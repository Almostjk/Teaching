{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words Model\n",
    "Bag of words model is very very simple. We will represent a tweet via a feature vector whose length is equal to the number of words in an english dictionary. Specifically, if a tweet contains the $i^\\textrm{th}$ word of the dictionary, then we will set $x_i=1$; otherwise we let $x_i=0$. For instance, the vector\n",
    "\n",
    "$$x = \\begin{pmatrix}1\\\\0\\\\0\\\\ \\vdots \\\\1 \\\\ \\vdots \\\\0\n",
    "\\end{pmatrix} \\quad \\begin{pmatrix}\\textrm{a}\\\\ \\textrm{aardvark} \\\\ \\textrm{aardwolf}\\\\ \\vdots \\\\ \\textrm{buy} \\\\ \\vdots \\\\ \\textrm{zygmurgy}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "is used to represent a tweet that contains the words “a” and “buy,” but not “aardvark,” “aardwolf” or “zygmurgy.”\n",
    "\n",
    "As a note, rather than using an english dictionary, it is typical to look through the training set and only use the words that occur in the training set. This is also good, because not all words in a tweet will be in a dictionary, in particular slang words. This method will also help in reduceing the dimensionality of the dataset, which will help reduce computational and space requirements.\n",
    "\n",
    "The set of words used in the feature vector is known as the vocabulary, and the dimension of the dataset is equal to the size of the vocabulary.\n",
    "\n",
    "### Beyond Bag of Words - Word2Vec\n",
    "One issue that the bag of word model has is that there is no notion of distance. Meaning, if we consider a bag of word model with 3 words `King, Queen, Man`. Then bag of word model will encode\n",
    "$$\\textrm{King}=(1, 0, 0) \\quad \\textrm{Queen}=(0, 1, 0) \\quad \\textrm{Man}=(0, 0, 1)$$\n",
    "\n",
    "One problem with such a simple system is that there is no meaning encoded in these vectors. In particular, we know that\n",
    "\n",
    "$$\\textrm{King} - \\textrm{Man} = \\textrm{Queen}$$\n",
    "\n",
    "Meaning that if you removed the man from the `King` you should get a `Queen`. But in the bag of words model you have\n",
    "\n",
    "$$\\textrm{King} - \\textrm{Man} = (1, 0, 0) - (0, 1, 0) = (1, -1, 0) \\neq \\textrm{Queen}$$\n",
    "\n",
    "More advanced models, like Word2Vec, begin to incorporate such ideas of relationships that exists between words. Another example of relationships that exist between words may be\n",
    "$$\\textrm{Concrete} + \\textrm{Bricks} = \\textrm{House}$$\n",
    "\n",
    "Other relationships may also be contextual/definition relationships. For example we expect the word `red` to be closer to `green` than it is to `King`. That is, we want\n",
    "\n",
    "$$|\\textrm{Red} - \\textrm{Green}| < |\\textrm{Red} - \\textrm{King}|$$\n",
    "Meaning, the distance for red to green is smaller than the distance from red to king. This is what is given by Word2Vec\n",
    "\n",
    "But using Bag of words, we have that\n",
    "$$|\\textrm{Red} - \\textrm{Green}| = |\\textrm{Red} - \\textrm{King}|$$\n",
    "\n",
    "\n",
    "\n",
    "I'll note, however, Word2Vec is quite advanced. But it is the state of the art when you want to convert words into vectors (or numbers). In fact, the latest and greatest language model GPT-3, uses Word2Vec.\n",
    "\n",
    "<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n",
    "\n",
    "### What is Token(isation)\n",
    "\n",
    "Tokenisation is the process of breaking up a string, in this case a tweet, into indiviual words. So if we had the sentence \n",
    "\n",
    "`There is an old Polish proverb that says that when your socks are not in your shoes don't look for them in heaven`\n",
    "\n",
    "Tokenisation will then split up the sentence into the idividual words, and will remove any punctuation (e.g. `!, ?, ., ;`) and capitals:\n",
    "\n",
    "`['there', 'is', 'an', 'old', 'polish', ..., 'for', 'them', 'in', 'heaven']`\n",
    "\n",
    "Tokenisation, however, actually goes further in this. In particular, there are useless words that have no use in analysis, known as stop words. Words like `is`, `and`, etc. get removed in the tokenisation process. So really you get\n",
    "\n",
    "`['there', 'old', 'polish', 'proverb', 'says', 'your', 'socks', 'shoes', 'dont', 'look', 'them', 'heaven']`\n",
    "\n",
    "Actually, tokenisation goes further. If we consider the words `socks` and `sock` then in the Bag of Word model these two would be considered as two different words:\n",
    "$$ \n",
    "\\begin{pmatrix}\\vdots \\\\ \\textrm{sock} \\\\ \\textrm{socks} \\\\ \\vdots \\end{pmatrix}\n",
    "$$\n",
    "But we know that there is no difference between the words `socks` and `sock`. So what we do now is to replace similar words (like `socks` and `sock`; or `shoes` and `shoe`) as the same words. THat is\n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix}\\vdots \\\\ \\textrm{sock} \\\\ \\textrm{socks}\\\\ \\vdots \\\\ \\textrm{camp} \\\\ \\textrm{camps} \\\\ \\textrm{camping} \\\\ \\textrm{camped} \\\\ \\vdots  \\end{pmatrix} \\implies \\begin{pmatrix}\\vdots \\\\ \\textrm{sock} \\\\ \\vdots \\\\ \\textrm{camp} \\\\ \\vdots \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In total, tokenisation will then convert the sentence\n",
    "\n",
    "`there is an old polish proverb that says that when your socks are not in your shoes don't look for them in heaven`\n",
    "\n",
    "to\n",
    "\n",
    "`['there', 'old', 'polish', 'proverb', 'say', 'your', 'sock', 'shoe', 'dont', 'look', 'them', 'heaven']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
