{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Correlation\n",
    "\n",
    "We know that if 2 predictors are highly correlated with each other then this can cause issues with the OLS solution. In particular let us say that we have 11 predictors. Of the 11 predictors $X_1$ and $X_2$ are highly correlated with each other. Since they are highly correlated it means that we can write\n",
    "\n",
    "$$\\large X_1 = \\beta_0 + \\beta_1 X_2 + \\varepsilon$$\n",
    "where $\\varepsilon$ is some noise. In other words we can use the predictor $X_2$ to estimate the value of $X_1$.\n",
    "\n",
    "# What is Multicollinearity\n",
    "However, high correlation is not the only situation that can affect the OLS solution. Multi-collinearity is another issue in which\n",
    "\n",
    "    Multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. \n",
    "    - https://www.investopedia.com/terms/m/multicollinearity.asp\n",
    "    \n",
    "    \n",
    "So instead, multi-collinearity is the situation where\n",
    "\n",
    "$$\\large X_1 = \\beta_0 + \\beta_1 X_2 + \\beta_2 X_5 + \\beta_3 X_7 + \\cdots + \\varepsilon$$\n",
    "where $\\varepsilon$ is some noise. In other words, we can use multiple predictors to estimate the value of $X_1$.\n",
    "\n",
    "The difficulty of multicollinearity is that it does not show up when looking at the correlation matrix. So lets see this here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "k = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(n, k)\n",
    "\n",
    "# Compute the weighted sum of x and add some random noise\n",
    "e = np.random.rand(n) * 1.1\n",
    "w = np.array([(-1) ** i for i in range(k)])\n",
    "x1 = (np.sum(x * w, axis=1) + e).reshape(n, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1, -1,  1, -1,  1, -1,  1, -1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374540</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.601115</td>\n",
       "      <td>0.708073</td>\n",
       "      <td>-1.154218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.969910</td>\n",
       "      <td>0.832443</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>0.183405</td>\n",
       "      <td>0.304242</td>\n",
       "      <td>0.524756</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>0.291229</td>\n",
       "      <td>0.185491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611853</td>\n",
       "      <td>0.139494</td>\n",
       "      <td>0.292145</td>\n",
       "      <td>0.366362</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>0.785176</td>\n",
       "      <td>0.199674</td>\n",
       "      <td>0.514234</td>\n",
       "      <td>0.592415</td>\n",
       "      <td>0.046450</td>\n",
       "      <td>1.260680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607545</td>\n",
       "      <td>0.170524</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>0.948886</td>\n",
       "      <td>0.965632</td>\n",
       "      <td>0.808397</td>\n",
       "      <td>0.304614</td>\n",
       "      <td>0.097672</td>\n",
       "      <td>0.684233</td>\n",
       "      <td>0.440152</td>\n",
       "      <td>0.966891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.122038</td>\n",
       "      <td>0.495177</td>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.909320</td>\n",
       "      <td>0.258780</td>\n",
       "      <td>0.662522</td>\n",
       "      <td>0.311711</td>\n",
       "      <td>0.520068</td>\n",
       "      <td>0.546710</td>\n",
       "      <td>0.184854</td>\n",
       "      <td>-0.611097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.374540  0.950714  0.731994  0.598658  0.156019  0.155995  0.058084   \n",
       "1  0.020584  0.969910  0.832443  0.212339  0.181825  0.183405  0.304242   \n",
       "2  0.611853  0.139494  0.292145  0.366362  0.456070  0.785176  0.199674   \n",
       "3  0.607545  0.170524  0.065052  0.948886  0.965632  0.808397  0.304614   \n",
       "4  0.122038  0.495177  0.034389  0.909320  0.258780  0.662522  0.311711   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.866176  0.601115  0.708073 -1.154218  \n",
       "1  0.524756  0.431945  0.291229  0.185491  \n",
       "2  0.514234  0.592415  0.046450  1.260680  \n",
       "3  0.097672  0.684233  0.440152  0.966891  \n",
       "4  0.520068  0.546710  0.184854 -0.611097  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.DataFrame(np.concatenate([x, x1], axis=1))\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.132448</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.045306</td>\n",
       "      <td>0.092790</td>\n",
       "      <td>-0.040598</td>\n",
       "      <td>0.072209</td>\n",
       "      <td>-0.016588</td>\n",
       "      <td>-0.095953</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.381040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.132448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212663</td>\n",
       "      <td>-0.085697</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>-0.025379</td>\n",
       "      <td>-0.176209</td>\n",
       "      <td>0.055295</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>-0.400067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.212663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>-0.155827</td>\n",
       "      <td>-0.006887</td>\n",
       "      <td>-0.114155</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.056831</td>\n",
       "      <td>0.119880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045306</td>\n",
       "      <td>-0.085697</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.101782</td>\n",
       "      <td>-0.119854</td>\n",
       "      <td>-0.071322</td>\n",
       "      <td>0.012184</td>\n",
       "      <td>-0.069192</td>\n",
       "      <td>-0.183094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.092790</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>-0.155827</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>0.019232</td>\n",
       "      <td>-0.107280</td>\n",
       "      <td>-0.148244</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.248976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.040598</td>\n",
       "      <td>-0.025379</td>\n",
       "      <td>-0.006887</td>\n",
       "      <td>0.101782</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>-0.107892</td>\n",
       "      <td>0.150582</td>\n",
       "      <td>0.088351</td>\n",
       "      <td>-0.244504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.072209</td>\n",
       "      <td>-0.176209</td>\n",
       "      <td>-0.114155</td>\n",
       "      <td>-0.119854</td>\n",
       "      <td>0.019232</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>-0.075692</td>\n",
       "      <td>0.432549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.016588</td>\n",
       "      <td>0.055295</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>-0.071322</td>\n",
       "      <td>-0.107280</td>\n",
       "      <td>-0.107892</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.115291</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>-0.324327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.095953</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.012184</td>\n",
       "      <td>-0.148244</td>\n",
       "      <td>0.150582</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>-0.115291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235775</td>\n",
       "      <td>0.182466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>0.056831</td>\n",
       "      <td>-0.069192</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.088351</td>\n",
       "      <td>-0.075692</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>0.235775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.303187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.381040</td>\n",
       "      <td>-0.400067</td>\n",
       "      <td>0.119880</td>\n",
       "      <td>-0.183094</td>\n",
       "      <td>0.248976</td>\n",
       "      <td>-0.244504</td>\n",
       "      <td>0.432549</td>\n",
       "      <td>-0.324327</td>\n",
       "      <td>0.182466</td>\n",
       "      <td>-0.303187</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.132448 -0.017471  0.045306  0.092790 -0.040598  0.072209   \n",
       "1  -0.132448  1.000000  0.212663 -0.085697  0.004565 -0.025379 -0.176209   \n",
       "2  -0.017471  0.212663  1.000000  0.032616 -0.155827 -0.006887 -0.114155   \n",
       "3   0.045306 -0.085697  0.032616  1.000000  0.146274  0.101782 -0.119854   \n",
       "4   0.092790  0.004565 -0.155827  0.146274  1.000000  0.041421  0.019232   \n",
       "5  -0.040598 -0.025379 -0.006887  0.101782  0.041421  1.000000  0.000513   \n",
       "6   0.072209 -0.176209 -0.114155 -0.119854  0.019232  0.000513  1.000000   \n",
       "7  -0.016588  0.055295  0.125099 -0.071322 -0.107280 -0.107892 -0.033213   \n",
       "8  -0.095953  0.038501  0.051094  0.012184 -0.148244  0.150582  0.007482   \n",
       "9   0.005863  0.115898  0.056831 -0.069192 -0.046883  0.088351 -0.075692   \n",
       "10  0.381040 -0.400067  0.119880 -0.183094  0.248976 -0.244504  0.432549   \n",
       "\n",
       "          7         8         9         10  \n",
       "0  -0.016588 -0.095953  0.005863  0.381040  \n",
       "1   0.055295  0.038501  0.115898 -0.400067  \n",
       "2   0.125099  0.051094  0.056831  0.119880  \n",
       "3  -0.071322  0.012184 -0.069192 -0.183094  \n",
       "4  -0.107280 -0.148244 -0.046883  0.248976  \n",
       "5  -0.107892  0.150582  0.088351 -0.244504  \n",
       "6  -0.033213  0.007482 -0.075692  0.432549  \n",
       "7   1.000000 -0.115291  0.019958 -0.324327  \n",
       "8  -0.115291  1.000000  0.235775  0.182466  \n",
       "9   0.019958  0.235775  1.000000 -0.303187  \n",
       "10 -0.324327  0.182466 -0.303187  1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8ff4633d10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(x_train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above there is no serious correlations between any of the predictors. We might be concered with the correlation between 10 & 1, 10 & 6. But otherwise it looks fine.\n",
    "\n",
    "So to find multicollinearity we can use multiple methods, but the most simple of it is the VIF - variance inflation factor. Again, multicollinearity is the situation in which\n",
    "\n",
    "$$\\large X_1 = \\beta_0 + \\beta_1 X_2 + \\beta_2 X_5 + \\beta_3 X_7 + \\cdots + \\varepsilon$$\n",
    "i.e. a simple linear regression model. So the method of VIF is simple, just build a linear regression model to predict $X_1$ and see what happens. Specifically, if we were to look at the VIF of $X_1$ we \n",
    "\n",
    "\n",
    "1. Perform the regression\n",
    "$$\\large X_1 = \\beta_0 + \\beta_1 X_2 + \\beta_2 X_3 + \\beta_3 X_4 + \\cdots + \\beta_{9} X_{10}+ \\beta_{10} X_{11}$$\n",
    "\n",
    "2. Compute the r-squared ($R_1$). Now lets think about this intuitively, if there did exist multicollinearity in $X_1$ then we expect the regression to perform well (i.e. high r-squared). But if there didn't exist multicollinearity then the regression will perform bad (i.e low r-squared).\n",
    "\n",
    "3. Compute the VIF. So if you have a high r-squared than the VIF will be high, and if you have a low r-squared then the VIF will be low.\n",
    "$$\\large VIF_1 = \\frac{1}{1-R_1}$$\n",
    "\n",
    "\n",
    "### Note\n",
    "Note that this only computes the VIF for 1 predictor. So you will need to do the same procedure for all the other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = 1\n",
    "\n",
    "# Dataframe of the target predictor\n",
    "target_predictor = x_train.loc[:, target_index]\n",
    "\n",
    "# Dataframe of everything else (removing the target predictor)\n",
    "other_predictors = x_train.drop(columns=target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.950714\n",
       "1    0.969910\n",
       "2    0.139494\n",
       "3    0.170524\n",
       "4    0.495177\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_predictor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374540</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.601115</td>\n",
       "      <td>0.708073</td>\n",
       "      <td>-1.154218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.832443</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>0.183405</td>\n",
       "      <td>0.304242</td>\n",
       "      <td>0.524756</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>0.291229</td>\n",
       "      <td>0.185491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611853</td>\n",
       "      <td>0.292145</td>\n",
       "      <td>0.366362</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>0.785176</td>\n",
       "      <td>0.199674</td>\n",
       "      <td>0.514234</td>\n",
       "      <td>0.592415</td>\n",
       "      <td>0.046450</td>\n",
       "      <td>1.260680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607545</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>0.948886</td>\n",
       "      <td>0.965632</td>\n",
       "      <td>0.808397</td>\n",
       "      <td>0.304614</td>\n",
       "      <td>0.097672</td>\n",
       "      <td>0.684233</td>\n",
       "      <td>0.440152</td>\n",
       "      <td>0.966891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.122038</td>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.909320</td>\n",
       "      <td>0.258780</td>\n",
       "      <td>0.662522</td>\n",
       "      <td>0.311711</td>\n",
       "      <td>0.520068</td>\n",
       "      <td>0.546710</td>\n",
       "      <td>0.184854</td>\n",
       "      <td>-0.611097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         2         3         4         5         6         7   \\\n",
       "0  0.374540  0.731994  0.598658  0.156019  0.155995  0.058084  0.866176   \n",
       "1  0.020584  0.832443  0.212339  0.181825  0.183405  0.304242  0.524756   \n",
       "2  0.611853  0.292145  0.366362  0.456070  0.785176  0.199674  0.514234   \n",
       "3  0.607545  0.065052  0.948886  0.965632  0.808397  0.304614  0.097672   \n",
       "4  0.122038  0.034389  0.909320  0.258780  0.662522  0.311711  0.520068   \n",
       "\n",
       "         8         9         10  \n",
       "0  0.601115  0.708073 -1.154218  \n",
       "1  0.431945  0.291229  0.185491  \n",
       "2  0.592415  0.046450  1.260680  \n",
       "3  0.684233  0.440152  0.966891  \n",
       "4  0.546710  0.184854 -0.611097  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the key is that we are going to use `other_predictor` to predict the `target_predictor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for Predictor 1: 0.57755\n",
      "VIF for Predictor 1: 2.36717\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(other_predictors, target_predictor)\n",
    "\n",
    "r2_score = model.score(other_predictors, target_predictor)\n",
    "\n",
    "vif = 1 / (1 - r2_score)\n",
    "print(f\"R2 for Predictor {target_index}: {round(r2_score, 5)}\")\n",
    "print(f\"VIF for Predictor {target_index}: {round(vif, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "R2 for Predictor 0: 0.49257\n",
      "VIF for Predictor 0: 1.97073\n",
      "----------------------------------------\n",
      "R2 for Predictor 1: 0.57755\n",
      "VIF for Predictor 1: 2.36717\n",
      "----------------------------------------\n",
      "R2 for Predictor 2: 0.53096\n",
      "VIF for Predictor 2: 2.13202\n",
      "----------------------------------------\n",
      "R2 for Predictor 3: 0.42663\n",
      "VIF for Predictor 3: 1.74407\n",
      "----------------------------------------\n",
      "R2 for Predictor 4: 0.51389\n",
      "VIF for Predictor 4: 2.05714\n",
      "----------------------------------------\n",
      "R2 for Predictor 5: 0.43896\n",
      "VIF for Predictor 5: 1.78241\n",
      "----------------------------------------\n",
      "R2 for Predictor 6: 0.48816\n",
      "VIF for Predictor 6: 1.95373\n",
      "----------------------------------------\n",
      "R2 for Predictor 7: 0.47124\n",
      "VIF for Predictor 7: 1.89122\n",
      "----------------------------------------\n",
      "R2 for Predictor 8: 0.54607\n",
      "VIF for Predictor 8: 2.20298\n",
      "----------------------------------------\n",
      "R2 for Predictor 9: 0.49707\n",
      "VIF for Predictor 9: 1.98834\n",
      "----------------------------------------\n",
      "R2 for Predictor 10: 0.89217\n",
      "VIF for Predictor 10: 9.27426\n"
     ]
    }
   ],
   "source": [
    "for target_index in range(n_cols):\n",
    "    target_predictor = x_train.loc[:, target_index]\n",
    "    other_predictors = x_train.drop(columns=target_index)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(other_predictors, target_predictor)\n",
    "    \n",
    "    r2_score = model.score(other_predictors, target_predictor)\n",
    "    vif = 1 / (1 - r2_score)\n",
    "    \n",
    "    print(\"-\"*40)\n",
    "    print(f\"R2 for Predictor {target_index}: {round(r2_score, 5)}\")\n",
    "    print(f\"VIF for Predictor {target_index}: {round(vif, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can see that the r-squared for predictor 10 is quite high - 89%. But the correlation matrix does not say so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.132448</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.045306</td>\n",
       "      <td>0.092790</td>\n",
       "      <td>-0.040598</td>\n",
       "      <td>0.072209</td>\n",
       "      <td>-0.016588</td>\n",
       "      <td>-0.095953</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.381040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.132448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212663</td>\n",
       "      <td>-0.085697</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>-0.025379</td>\n",
       "      <td>-0.176209</td>\n",
       "      <td>0.055295</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>-0.400067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.212663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>-0.155827</td>\n",
       "      <td>-0.006887</td>\n",
       "      <td>-0.114155</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.056831</td>\n",
       "      <td>0.119880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045306</td>\n",
       "      <td>-0.085697</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.101782</td>\n",
       "      <td>-0.119854</td>\n",
       "      <td>-0.071322</td>\n",
       "      <td>0.012184</td>\n",
       "      <td>-0.069192</td>\n",
       "      <td>-0.183094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.092790</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>-0.155827</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>0.019232</td>\n",
       "      <td>-0.107280</td>\n",
       "      <td>-0.148244</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.248976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.040598</td>\n",
       "      <td>-0.025379</td>\n",
       "      <td>-0.006887</td>\n",
       "      <td>0.101782</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>-0.107892</td>\n",
       "      <td>0.150582</td>\n",
       "      <td>0.088351</td>\n",
       "      <td>-0.244504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.072209</td>\n",
       "      <td>-0.176209</td>\n",
       "      <td>-0.114155</td>\n",
       "      <td>-0.119854</td>\n",
       "      <td>0.019232</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>-0.075692</td>\n",
       "      <td>0.432549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.016588</td>\n",
       "      <td>0.055295</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>-0.071322</td>\n",
       "      <td>-0.107280</td>\n",
       "      <td>-0.107892</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.115291</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>-0.324327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.095953</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.012184</td>\n",
       "      <td>-0.148244</td>\n",
       "      <td>0.150582</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>-0.115291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235775</td>\n",
       "      <td>0.182466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>0.056831</td>\n",
       "      <td>-0.069192</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.088351</td>\n",
       "      <td>-0.075692</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>0.235775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.303187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.381040</td>\n",
       "      <td>-0.400067</td>\n",
       "      <td>0.119880</td>\n",
       "      <td>-0.183094</td>\n",
       "      <td>0.248976</td>\n",
       "      <td>-0.244504</td>\n",
       "      <td>0.432549</td>\n",
       "      <td>-0.324327</td>\n",
       "      <td>0.182466</td>\n",
       "      <td>-0.303187</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.132448 -0.017471  0.045306  0.092790 -0.040598  0.072209   \n",
       "1  -0.132448  1.000000  0.212663 -0.085697  0.004565 -0.025379 -0.176209   \n",
       "2  -0.017471  0.212663  1.000000  0.032616 -0.155827 -0.006887 -0.114155   \n",
       "3   0.045306 -0.085697  0.032616  1.000000  0.146274  0.101782 -0.119854   \n",
       "4   0.092790  0.004565 -0.155827  0.146274  1.000000  0.041421  0.019232   \n",
       "5  -0.040598 -0.025379 -0.006887  0.101782  0.041421  1.000000  0.000513   \n",
       "6   0.072209 -0.176209 -0.114155 -0.119854  0.019232  0.000513  1.000000   \n",
       "7  -0.016588  0.055295  0.125099 -0.071322 -0.107280 -0.107892 -0.033213   \n",
       "8  -0.095953  0.038501  0.051094  0.012184 -0.148244  0.150582  0.007482   \n",
       "9   0.005863  0.115898  0.056831 -0.069192 -0.046883  0.088351 -0.075692   \n",
       "10  0.381040 -0.400067  0.119880 -0.183094  0.248976 -0.244504  0.432549   \n",
       "\n",
       "          7         8         9         10  \n",
       "0  -0.016588 -0.095953  0.005863  0.381040  \n",
       "1   0.055295  0.038501  0.115898 -0.400067  \n",
       "2   0.125099  0.051094  0.056831  0.119880  \n",
       "3  -0.071322  0.012184 -0.069192 -0.183094  \n",
       "4  -0.107280 -0.148244 -0.046883  0.248976  \n",
       "5  -0.107892  0.150582  0.088351 -0.244504  \n",
       "6  -0.033213  0.007482 -0.075692  0.432549  \n",
       "7   1.000000 -0.115291  0.019958 -0.324327  \n",
       "8  -0.115291  1.000000  0.235775  0.182466  \n",
       "9   0.019958  0.235775  1.000000 -0.303187  \n",
       "10 -0.324327  0.182466 -0.303187  1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the data I have here actually is\n",
    "\n",
    "$$X_{10} = X_0 - X_1 + X_2 - X_3 + X_4 - X_5 + X_6 - X_7 + X_8 - X_9 + \\varepsilon$$\n",
    "wher $\\varepsilon$ is some noise. We can see how close our regression got to this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index = 10\n",
    "\n",
    "target_predictor = x_train.loc[:, target_index]\n",
    "other_predictors = x_train.drop(columns=target_index)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(other_predictors, target_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04471965, -1.24659152,  1.13780443, -1.01446678,  1.07244312,\n",
       "       -0.94176956,  1.07009378, -1.04350179,  1.08759064, -1.06551222])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04471965, -1.24659152,  1.13780443, -1.01446678,  1.07244312,\n",
       "       -0.94176956,  1.07009378, -1.04350179,  1.08759064, -1.06551222])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_coefficients = model.coef_\n",
    "predictor_labels = [f\"X_{i}\" for i in range(10)]\n",
    "\n",
    "predictor_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04X_0 + -1.25X_1 + 1.14X_2 + -1.01X_3 + 1.07X_4 + -0.94X_5 + 1.07X_6 + -1.04X_7 + 1.09X_8 + -1.07X_9\n"
     ]
    }
   ],
   "source": [
    "formated_coeff_label = [f\"{round(coeff, 2)}{label}\" \n",
    "                        for coeff, label in zip(predictor_coefficients, predictor_labels)]\n",
    "\n",
    "print(\" + \".join(formated_coeff_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large 1.04X_0 -1.25X_1 + 1.14X_2 -1.01X_3 + 1.07X_4 -0.94X_5 + 1.07X_6 -1.04X_7 + 1.09X_8 -1.07X_9$$\n",
    "\n",
    "Pretty close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.970732\n",
       "1     2.367170\n",
       "2     2.132022\n",
       "3     1.744068\n",
       "4     2.057140\n",
       "5     1.782413\n",
       "6     1.953732\n",
       "7     1.891223\n",
       "8     2.202976\n",
       "9     1.988335\n",
       "10    9.274260\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = add_constant(x_train)\n",
    "pd.Series([variance_inflation_factor(X.values, i) for i in range(1, X.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "R2 for Predictor 0: 0.49257\n",
      "VIF for Predictor 0: 1.97073\n",
      "----------------------------------------\n",
      "R2 for Predictor 1: 0.57755\n",
      "VIF for Predictor 1: 2.36717\n",
      "----------------------------------------\n",
      "R2 for Predictor 2: 0.53096\n",
      "VIF for Predictor 2: 2.13202\n",
      "----------------------------------------\n",
      "R2 for Predictor 3: 0.42663\n",
      "VIF for Predictor 3: 1.74407\n",
      "----------------------------------------\n",
      "R2 for Predictor 4: 0.51389\n",
      "VIF for Predictor 4: 2.05714\n",
      "----------------------------------------\n",
      "R2 for Predictor 5: 0.43896\n",
      "VIF for Predictor 5: 1.78241\n",
      "----------------------------------------\n",
      "R2 for Predictor 6: 0.48816\n",
      "VIF for Predictor 6: 1.95373\n",
      "----------------------------------------\n",
      "R2 for Predictor 7: 0.47124\n",
      "VIF for Predictor 7: 1.89122\n",
      "----------------------------------------\n",
      "R2 for Predictor 8: 0.54607\n",
      "VIF for Predictor 8: 2.20298\n",
      "----------------------------------------\n",
      "R2 for Predictor 9: 0.49707\n",
      "VIF for Predictor 9: 1.98834\n",
      "----------------------------------------\n",
      "R2 for Predictor 10: 0.89217\n",
      "VIF for Predictor 10: 9.27426\n"
     ]
    }
   ],
   "source": [
    "for target_index in range(n_cols):\n",
    "    target_predictor = x_train.loc[:, target_index]\n",
    "    other_predictors = x_train.drop(columns=target_index)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(other_predictors, target_predictor)\n",
    "    \n",
    "    r2_score = model.score(other_predictors, target_predictor)\n",
    "    vif = 1 / (1 - r2_score)\n",
    "    \n",
    "    print(\"-\"*40)\n",
    "    print(f\"R2 for Predictor {target_index}: {round(r2_score, 5)}\")\n",
    "    print(f\"VIF for Predictor {target_index}: {round(vif, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
