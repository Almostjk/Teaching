{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img align=\"center\" src=\"http://sydney.edu.au/images/content/about/logo-mono.jpg\">\n",
    "</center>\n",
    "<h1 align=\"center\" style=\"margin-top:10px\">Statistical Learning and Data Mining</h1>\n",
    "<h2 align=\"center\" style=\"margin-top:20px\">Week 13 Tutorial: Deep Learning with PyTorch</h2>\n",
    "<br>\n",
    "\n",
    "This tutorial is an introduction to building and training neural networks with [PyTorch](https://pytorch.org/). We'll build a simple feedforward network for fraud detection and train it by stochastic gradient descent.\n",
    "\n",
    "<a href=\"#1.-Credit-Card-Fraud-Data\">Credit card fraud data</a> <br>\n",
    "<a href=\"#2.-Dataset-and-DataLoader\">Dataset and DataLoader</a> <br>\n",
    "<a href=\"#3.-Building-a-neural-network\">Building a neural network</a> <br>\n",
    "<a href=\"#4.-Training\">Training</a> <br>\n",
    "<a href=\"#5.-Logistic-Regression\">Logistic regression</a> <br>\n",
    "<a href=\"#6.-Validation-results\">Validation results</a> <br>\n",
    "\n",
    "This notebook relies on the following libraries and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package versions: \n",
      "\n",
      "numpy 1.20.3\n",
      "scipy 1.7.3\n",
      "pandas 1.2.5\n",
      "scikit-learn 1.0.2\n",
      "torch 1.10.0\n"
     ]
    }
   ],
   "source": [
    "# Requirements \n",
    "\n",
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch \n",
    "\n",
    "print(f'Package versions: \\n')\n",
    "\n",
    "print(f'numpy {np.__version__}')\n",
    "print(f'scipy {scipy.__version__}')\n",
    "print(f'pandas {pd.__version__}')\n",
    "print(f'scikit-learn {sklearn.__version__}')\n",
    "print(f'torch {torch.__version__}')\n",
    "\n",
    "# Notebook tested on:\n",
    "\n",
    "# numpy 1.21.5\n",
    "# scipy 1.7.1\n",
    "# pandas 1.3.4\n",
    "# scikit-learn 1.0.2\n",
    "# torch 1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional configuration\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Credit card fraud data\n",
    "\n",
    "This tutorial will be based on the [credit card fraud dataset](hhttps://www.kaggle.com/mlg-ulb/creditcardfraud) available from [Kaggle Datasets](https://www.kaggle.com/datasets). Our objective is to detect fraudulent credit card transactions using classification methods. \n",
    "\n",
    "Let's assume the following loss matrix: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Actual/Predicted</th>\n",
    "    <th>Legitimate</th>\n",
    "     <th>Fraud</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Legitimate</th>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Fraud</th>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "That is, we assume that it is much worse for the financial institution to miss a fraudulent transaction than to flag a legitimate transaction as potential fraud.\n",
    "\n",
    "We start by loading and inspecting the data. All features except the transaction amount are the result of a principal components analysis (PCA) transformation of undisclosed predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively large dataset with 284,807 transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are highly imbalanced: only 492 transactions (0.17%) are fraudulent.  This makes the problem much more challenging than the total number of observations would suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so few observations in the fraud class, we should ideally use cross-validation throughout the analysis. However, this would excessively complicate the code, and we simply create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'Class'\n",
    "index_train, index_val = train_test_split(np.array(data.index), stratify=data[response], \n",
    "                                          train_size=0.8, random_state=1)\n",
    "\n",
    "predictors = list(data.columns[1:-1])  # we won't use the  time variable\n",
    "\n",
    "X_train = data.loc[index_train, predictors].to_numpy()\n",
    "y_train = data.loc[index_train, response].to_numpy()\n",
    "\n",
    "X_valid = data.loc[index_val, predictors].to_numpy()\n",
    "y_valid = data.loc[index_val, response].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step that we need to prepare the data is to standardise the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader\n",
    "\n",
    "When working with PyTorch, we need to tell it how to process the data and construct minibatches for stochastic gradient descent. \n",
    "\n",
    "The first step is to create a PyTorch dataset object. The easiest way to do this in our case is to convert the original NumPy arrays into PyTorch [tensors](https://pytorch.org/docs/stable/tensors.html), the format required by the package, and then use the [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) function to create the dataset.\n",
    "\n",
    "A tensor in this context is just another name for an array. The PyTorch documentation writes:\n",
    "\n",
    "> A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "validset = TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In many cases, you'll need to create a custom dataset. A PyTorch dataset must be a class that implements three methods: `__init__`, `__len__`, and `__getitem__`. The first takes data as an input, processes it as required, and instantiates the `DataSet` object. The second returns the number of observations. The third takes an index as an input and returns the observation that corresponds to that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.input = torch.from_numpy(X).float()\n",
    "        self.target = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx, :], self.target[idx]\n",
    "    \n",
    "trainset = FraudDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) takes a `DataSet` as an input and creates an iterator that allows PyTorch to process the data in batches.\n",
    "\n",
    " Setting the `shuffle` option to `True` makes the DataLoader reshuffle the data at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell grabs a randomly sampled mini-batch for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1361,  0.6230,  0.3034,  ..., -0.0970, -0.2962, -0.3064],\n",
       "        [-0.8357,  0.3011,  0.9927,  ..., -0.1898, -0.0802,  0.1593],\n",
       "        [-0.4191,  0.6526,  0.7345,  ...,  0.5310,  0.4235, -0.1149],\n",
       "        ...,\n",
       "        [-1.6528,  0.4803,  0.5746,  ..., -1.0794, -1.3220,  0.0580],\n",
       "        [-0.3112,  0.2711,  0.8499,  ..., -0.8057, -1.0689, -0.2748],\n",
       "        [-0.1912,  0.3677, -1.1958,  ..., -0.9205, -1.0006, -0.1599]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(trainloader))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a neural network\n",
    "\n",
    "Our neural network model will be a multilayer perceptron (MLP) with three hidden layers. Each layer will have 128 hidden units and the activation function will be the rectified linear unit (ReLU).\n",
    "\n",
    "We code the model as a PyTorch neural network module that is a subclass of [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The model class needs to have an `__init__` method that initialises the neural network layers and a `forward` method that implements the operations to be performed on the inputs.\n",
    "\n",
    "The following code takes advantage of the `nn.Sequential` class, which allows us to quickly stack standard layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 29)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "   \n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.feedforward = nn.Sequential(            \n",
    "            nn.Linear(29, 128),  # 28 -> 128 -> ReLU -> 128 -> ReLU -> 128 -> ReLU -> 1 -> Sigmoid\n",
    "            nn.ReLU(),                       \n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )                        \n",
    "\n",
    "    def forward(self, X):        \n",
    "        return self.feedforward(X).flatten() # returns a flat array as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (feedforward): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNetwork()\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torchinfo](https://github.com/TylerYep/torchinfo) package provides a more detailed model summary.  We need to pass the input size (including the batch size) for it to work out how each layer processes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NeuralNetwork                            --                        --\n",
       "â”œâ”€Sequential: 1-1                        [64, 1]                   --\n",
       "â”‚    â””â”€Linear: 2-1                       [64, 128]                 3,840\n",
       "â”‚    â””â”€ReLU: 2-2                         [64, 128]                 --\n",
       "â”‚    â””â”€Linear: 2-3                       [64, 128]                 16,512\n",
       "â”‚    â””â”€ReLU: 2-4                         [64, 128]                 --\n",
       "â”‚    â””â”€Linear: 2-5                       [64, 128]                 16,512\n",
       "â”‚    â””â”€ReLU: 2-6                         [64, 128]                 --\n",
       "â”‚    â””â”€Linear: 2-7                       [64, 1]                   129\n",
       "â”‚    â””â”€Sigmoid: 2-8                      [64, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 36,993\n",
       "Trainable params: 36,993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.37\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.20\n",
       "Params size (MB): 0.15\n",
       "Estimated Total Size (MB): 0.35\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mlp, input_size=(64, 29))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Computing predictions\n",
    "\n",
    "At the most basic level, computing the output of the model given input data is just a matter of running a command such as `output = model(X)`. \n",
    "\n",
    "In practice, we need to handle a few practical details such as moving the data to the GPU memory. We write a function to take care of the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58867204, -0.00190627, -0.15430595, ..., -0.02083907,\n",
       "        -0.03400482, -0.11962486],\n",
       "       [-0.46600498, -0.59279613,  0.68102923, ..., -0.32153323,\n",
       "        -0.48878674,  0.21434073],\n",
       "       [ 0.62651986,  0.30118025, -0.05618417, ..., -0.01819322,\n",
       "         0.01123113, -0.32284392],\n",
       "       ...,\n",
       "       [ 1.01467248,  0.06443548, -0.61140049, ...,  0.18057578,\n",
       "        -0.01692782, -0.32320388],\n",
       "       [ 0.96789021, -0.12132428, -0.26700661, ...,  0.12197599,\n",
       "        -0.10052038, -0.32020419],\n",
       "       [ 0.53491683, -1.0159783 , -0.26833026, ..., -0.12881622,\n",
       "         0.1339857 ,  0.70081061]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, X):\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Move data to device\n",
    "    X_g = X.to(device)\n",
    "    \n",
    "    # Put model on evaluation mode (it makes no difference but needed in some cases)\n",
    "    net.eval()\n",
    "    \n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Predicted probabilities \n",
    "        # the .cpu().detach() part transfers the result to the cpu\n",
    "        output = net(X_g).cpu().detach()\n",
    "    \n",
    "    return output # the output is a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training\n",
    "\n",
    "Let's train our neural network! \n",
    "\n",
    "So far in this unit, we trained our models using simple commands such as `model.fit(X_train, y_train)`. Training models with PyTorch more coding, so we implement a function to train the model. I annotated all the steps are annotated so that you can understand the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, trainloader, validset, num_epochs = 5 , lr = 1e-3):\n",
    "    \n",
    "    # Get device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Instantiate model and move to device\n",
    "    net = model().to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = nn.BCELoss() # binary cross-entropy loss, assumes that the output of the network is a probability\n",
    "    \n",
    "    # Instantiate optimiser\n",
    "    # Adam is a variant of SGD that often works well for training neural networks\n",
    "    # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "    optimiser = torch.optim.Adam(net.parameters(), lr=lr) \n",
    "    \n",
    "    # Addding a learning rate scheduler to improve training\n",
    "    # Adam + OneCycleLR is a good default for many problems\n",
    "    # Learn more: https://sgugger.github.io/the-1cycle-policy.html\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=lr, \n",
    "                                                    steps_per_epoch=len(trainloader), epochs=num_epochs,\n",
    "                                                    three_phase=True)\n",
    "    # Number of training samples\n",
    "    num_samples = len(trainloader.dataset)\n",
    "    \n",
    "    # Initialise table to track training\n",
    "    table = init_training_table(num_epochs)\n",
    "    \n",
    "    # Training loop\n",
    "    print('Running first epoch')\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Make sure that the model is on training mode\n",
    "        net.train()\n",
    "        \n",
    "        # Initialise timer\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Initialise metric\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Iterate over minibatches\n",
    "        for X, y in trainloader:\n",
    "\n",
    "            # Move minibatch to device\n",
    "            X_g = X.to(device)\n",
    "            y_g = y.to(device)\n",
    "\n",
    "            # Reset the gradient\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Compute predictions\n",
    "            output = net(X_g)\n",
    "\n",
    "            # Evaluate cost function\n",
    "            loss = loss_fn(output, y_g)\n",
    "\n",
    "            # Compute gradient \n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Keep track of the training loss\n",
    "            l = loss.cpu().detach().numpy()\n",
    "            train_loss +=  l*(len(y)/num_samples)\n",
    "  \n",
    "        # Epoch length\n",
    "        duration = time.time() - epoch_start \n",
    "        \n",
    "        # Display metrics\n",
    "        table.iloc[epoch, 1] = np.round(10*train_loss, 3)\n",
    "        table = update_training_table(table, net, validset, epoch, duration)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is auxiliary code to display the training progress. This code is not important, so let's just run it and proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary code\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import recall_score, average_precision_score\n",
    "\n",
    "def evaluate(net, validset):\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Set model to evaluation mode (not necessary here but required in general)\n",
    "    net.eval()\n",
    "    \n",
    "    # Get input and target\n",
    "    X, y = validset[:]\n",
    "        \n",
    "    # Predicted probabilities \n",
    "    output = predict(net, X)\n",
    "        \n",
    "    # Validation loss\n",
    "    loss = F.binary_cross_entropy(output, y).item()\n",
    "    \n",
    "    # Convert output to numpy\n",
    "    y_prob = output.numpy()\n",
    "     \n",
    "    # Classification using the decision threshold\n",
    "    tau = 1/11\n",
    "    y_pred = (y_prob > tau).astype(int)\n",
    "    \n",
    "    # Validation metrics\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    average_precision = average_precision_score(y_valid, y_prob)\n",
    "    \n",
    "    return loss, recall, average_precision \n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def init_training_table(num_epochs):\n",
    "    table = pd.DataFrame(np.arange(1, num_epochs+1), columns = ['epoch'])\n",
    "    table['train loss'] = 0.0\n",
    "    table['valid loss'] = 0.0\n",
    "    table['valid recall'] = 0.0\n",
    "    table['valid average precision'] = 0.0\n",
    "    table['time'] = ''\n",
    "    return table\n",
    "\n",
    "def update_training_table(table, net, validset, epoch, duration):\n",
    "    \n",
    "    # Run evaluation function to get validation metrics\n",
    "    valid_loss, valid_recall, valid_ave_precision = evaluate(net, validset)\n",
    "        \n",
    "    # Update table\n",
    "    table.iloc[epoch, 2] = np.round(10*valid_loss, 3)\n",
    "    table.iloc[epoch, 3] = np.round(valid_recall, 3)\n",
    "    table.iloc[epoch, 4] = np.round(valid_ave_precision, 3)\n",
    "     \n",
    "    # Epoch length   \n",
    "    if duration > 3600:\n",
    "        table.iloc[epoch, 5] = time.strftime('%H:%M:%S', time.gmtime(duration))\n",
    "    else:\n",
    "        table.iloc[epoch, 5] = time.strftime('%M:%S', time.gmtime(duration))\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    display(HTML(table.iloc[:epoch+1, :].to_html(index=False)))\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a good learning rate is critical in deep learning. Fortunately, the [learning rate finder](https://github.com/davidtvs/pytorch-lr-finder) proposed in [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186) by Leslie N. Smith can help us with this.\n",
    "\n",
    "The idea is to start training the model with a learning rates that range from very small to somewhat large. We then plot the training error against the learning rate and pick a value before the minimum. We want to choose a value in a range where there's a good improvement in the loss, but not so high that the learning rate may cause instability.  The minimum is too high since it's on the edge of instability, so we choose a learning rate one order of magnitude below.\n",
    "\n",
    "See this [blog post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate) for a detailed explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4546273ba15743a4b6d68a37e097dfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 2.85E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_fn =  nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "lr_finder = LRFinder(model, optimiser, loss_fn, device = device)\n",
    "lr_finder.range_test(trainloader, end_lr=1, num_iter=100)\n",
    "lr_finder.plot() \n",
    "lr_finder.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train loss</th>\n",
       "      <th>valid loss</th>\n",
       "      <th>valid recall</th>\n",
       "      <th>valid average precision</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.704</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.850</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.879</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.882</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.891</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.890</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.890</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.890</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = train(NeuralNetwork, trainloader, validset, num_epochs=10, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will not get the same numbers because of the randomness in the learning algorithm. When using the CPU for training, it's possible to achieve reproducibility by following the [recommendations](https://pytorch.org/docs/stable/notes/randomness.html) in the PyTorch documentation. With GPU training, reproducibility is difficult and often not possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=50, scoring='neg_log_loss')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# No regularisation\n",
    "logit = LogisticRegression(penalty='none', solver='lbfgs')\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# L2 regularisation\n",
    "# logit_l2= LogisticRegressionCV(Cs = 50, penalty='l2', solver='lbfgs', scoring='neg_log_loss', n_jobs=-1)\n",
    "logit_l2= LogisticRegressionCV(Cs=50, penalty='l2', solver='lbfgs', scoring='neg_log_loss')\n",
    "logit_l2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validation results\n",
    "\n",
    "The next cell compares the validation performance of the neural network against the logistic regression benchmark. The neural network outperforms the logistic regression in terms of the actual loss, average precision, and cross-entropy. \n",
    "\n",
    "Some important comments:\n",
    "\n",
    "(i) As noted above, you will not get the same numbers because of the randomness in the optimisation process. \n",
    "\n",
    "(ii) A useful trick for training neural networks is to re-run the learning algorithm if necessary. You can then select and save a model that performs well on the validation set. The disadvantage of this approach is that it can overfit the validation set. If possible, it's better to average multiple neural networks trained on different training-validation splits, discarding those with poor validation performance. \n",
    "\n",
    "(iii) The comparison with the logistic regression is not entirely rigorous since we looked at the validation performance to select a reasonable number of epochs for training the neural network.\n",
    "\n",
    "(iv) It's not clear why the neural networks performs better. As earlier in the unit, it's important to use EDA and interpretability tools to understand what's happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual loss</th>\n",
       "      <th>Error rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Cross-entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.827</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic $\\ell_2$</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.827</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural network</th>\n",
       "      <td>0.695</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Actual loss  Error rate  Sensitivity  Specificity  \\\n",
       "Logistic                 1.000       0.001        0.827        1.000   \n",
       "Logistic $\\ell_2$        1.000       0.001        0.827        1.000   \n",
       "Neural network           0.695       0.001        0.898        0.999   \n",
       "\n",
       "                   Precision  Average Precision  Cross-entropy  \n",
       "Logistic               0.802              0.811          0.033  \n",
       "Logistic $\\ell_2$      0.802              0.815          0.033  \n",
       "Neural network         0.733              0.890          0.021  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, log_loss\n",
    "\n",
    "columns=['Actual loss', 'Error rate', 'Sensitivity', 'Specificity', \n",
    "         'Precision', 'Average Precision', 'Cross-entropy']\n",
    "rows=['Logistic', 'Logistic $\\ell_2$', 'Neural network']\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "methods=[logit, logit_l2, None]\n",
    "\n",
    "lfp = 1\n",
    "lfn = 10\n",
    "tau = lfp/(lfp+lfn)\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    if i<2:\n",
    "        y_prob = method.predict_proba(X_valid)[:, 1]\n",
    "    else:\n",
    "        X = validset[:][0]\n",
    "        y_prob = predict(mlp, X).numpy()\n",
    "        y_prob[y_prob < 1e-5] = 1e-5 # Log-loss returns NaN if too close to zero or one\n",
    "        y_prob[y_prob > 1 - 1e-5] =  1- 1e-5\n",
    "        \n",
    "\n",
    "    y_pred = (y_prob > tau).astype(int)\n",
    "       \n",
    "    tn, fp, fn, tp = confusion_matrix(y_valid, y_pred).ravel()\n",
    "    \n",
    "    results.iloc[i,0] = (fp*lfp+fn*lfn)/len(y_valid)\n",
    "    results.iloc[i,1] = 1 - accuracy_score(y_valid, y_pred)\n",
    "    results.iloc[i,2] = tp/(tp+fn)\n",
    "    results.iloc[i,3] = tn/(tn+fp)\n",
    "    results.iloc[i,4] = precision_score(y_valid, y_pred)\n",
    "    results.iloc[i,5] = average_precision_score(y_valid, y_prob)\n",
    "    results.iloc[i,6] = 10*log_loss(y_valid, y_prob)\n",
    "\n",
    "results.iloc[:,0] /= results.iloc[0,0]\n",
    "results.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
