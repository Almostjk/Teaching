{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"http://sydney.edu.au/images/content/about/logo-mono.jpg\">\n",
    "<h1 align=\"center\" style=\"margin-top:10px\">Statistical Learning and Data Mining</h1>\n",
    "<h2 align=\"center\" style=\"margin-top:20px\">Tutorial 10 (Extra): Stochastic Gradient Descent</h2>\n",
    "<br>\n",
    "\n",
    "This notebook is a demonstration of stochastic gradient descent with the Keras package.\n",
    "\n",
    "<a href=\"#1.-Credit-card-fraud-data\">Credit card fraud data</a> <br>\n",
    "<a href=\"#2.-Feature-Engineering\">Feature engineering</a> <br>\n",
    "<a href=\"#3.-Neural-Network\">Stochastic gradient descent</a> <br>\n",
    "<a href=\"#7.-Model-validation\">Model Validation</a> <br>\n",
    "\n",
    "This notebook relies on the following libraries and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "sns.set_context('notebook') # optimises figures for notebook display\n",
    "sns.set_style('ticks') # set default plot style\n",
    "crayon = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', \n",
    "          '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(crayon) # set custom color scheme\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Credit card fraud data\n",
    "\n",
    "We use the [credit card fraud dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) data available from [Kaggle Datasets](https://www.kaggle.com/datasets). Our objective is to predict which credit card transactions are fraud. Let's assume the following loss matrix: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Actual/Predicted</th>\n",
    "    <th>Legitimate</th>\n",
    "     <th>Fraud</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Legitimate</th>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Fraud</th>\n",
    "    <td>50</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "That is, we assume that it is much worse for the financial institution to not catch a fraud than to flag a legitimate transaction as potential fraud.\n",
    "\n",
    "We start as usual by loading and inspecting the data (the predictors are de-identified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 284,807 transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the large number of observations, only 492 transactions (0.17%) are fraud. Therefore this is a highly imbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the number of frauds is small we only consider training and validation sets for this tutorial, without no test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response='Class'\n",
    "index_train, index_val  = train_test_split(np.array(data.index), stratify=data[response], train_size=0.8, random_state=1)\n",
    "\n",
    "predictors = list(data.columns[1:-1])\n",
    "\n",
    "X_train = data.loc[index_train, predictors].to_numpy()\n",
    "y_train = data.loc[index_train, response].to_numpy()\n",
    "\n",
    "X_val = data.loc[index_val, predictors].to_numpy()\n",
    "y_val = data.loc[index_val, response].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "We standardise the predictors and perform a Yeo-Johnson transformation on all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "yj = PowerTransformer(method='yeo-johnson')\n",
    "yj.fit(X_train)\n",
    "\n",
    "X_train = yj.transform(X_train)\n",
    "X_val = yj.transform(X_val)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stochastic gradient descent\n",
    "\n",
    "This dataset represents only two days of transactions for the company that supplied the data. Therefore, in a real setting, we could be fitting a classifier for fraud detection on hundreds of millions of transactions. With such large data, we often turn to stochastic gradient descent (SGD) for optimisation due to the high computational cost of calculating the gradient for the entire training data.\n",
    "\n",
    "Even though scikit-learn has an implementation of SGD, the [Keras](https://keras.io/) package for deep learning is more popular for this purpose. You will study deep learning if you take QBUS6850 (Machine Learning in Business). Here, we exploit the fact that the logistic regression model is a special case of deep learning.\n",
    "\n",
    "The next cell specifies the logistic regression model as a Keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 29)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Architecture\n",
    "X = keras.Input(shape=(X_train.shape[1],))\n",
    "y = layers.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "# Build model\n",
    "sgd = keras.Model(inputs=X, outputs=y)\n",
    "sgd.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Adam algorithm for optimisation, which is an improved version of SGD that does not require much tuning\n",
    "\n",
    "We commonly use a technique called early stopping when training a model by SGD. In early stopping, we keep track of the validation error at every epoch (or at every few epochs) and stop the optimisation algorithm if the validation error stops improving. Early stopping works as a regularisation technique, with effect similar to $\\ell_2$ regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e9d9f6ed08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# A callback is an object that implements actions at different stages of training (typically at every epoch)\n",
    "# This callback will stop training when the validation error does not improve for 5 epochs\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "\n",
    "# Optimisation\n",
    "sgd.compile(loss='binary_crossentropy', optimizer=Adam(1e-2))\n",
    "sgd.fit(X_train, \n",
    "        y_train, \n",
    "        epochs=50,\n",
    "        batch_size=512, \n",
    "        validation_data =(X_val, y_val), \n",
    "        callbacks = [callback],\n",
    "        verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation errors stabilise after just a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax= plt.subplots()\n",
    "ax.plot(sgd.history.history['loss'], label='Training error')\n",
    "ax.plot(sgd.history.history['val_loss'], label='Validation error')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(frameon=False)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='none',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(penalty='none')\n",
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Validation\n",
    "\n",
    "We end up with two versions of the logistic regression model trained different optimisation algorithms. While we would need more fraud cases in the validation set to reach a firm conclusion, the early stopping in SGD may have led to an improvement in the risk (via higher sensitivity). Your numbers for SGD may be different due to the randomness in the method. \n",
    "\n",
    "In a real-world setting, small reductions in the risk can translate into significant reductions in financial losses due to fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relative risk</th>\n",
       "      <th>Error rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Average Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression (SGD)</th>\n",
       "      <td>0.867</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Relative risk  Error rate  Sensitivity  \\\n",
       "Logistic regression                1.000       0.002        0.888   \n",
       "Logistic regression (SGD)          0.867       0.001        0.898   \n",
       "\n",
       "                           Specificity  Precision  Average Precision  \n",
       "Logistic regression              0.998      0.500              0.828  \n",
       "Logistic regression (SGD)        0.999      0.629              0.823  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, log_loss\n",
    "from sklearn.metrics import confusion_matrix, log_loss, average_precision_score, f1_score\n",
    "\n",
    "columns=['Relative risk', 'Error rate', 'Sensitivity', 'Specificity', \n",
    "         'Precision', 'Average Precision']\n",
    "rows=['Logistic regression', 'Logistic regression (SGD)']\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "methods=[logit, sgd]\n",
    "\n",
    "lfp = 1\n",
    "lfn = 50\n",
    "tau = lfp/(lfp+lfn)\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    if method in [sgd]:\n",
    "        y_prob = method.predict(X_val)\n",
    "    else:\n",
    "        y_prob = method.predict_proba(X_val)[:,1]\n",
    "\n",
    "    y_pred = (y_prob>tau).astype(int)\n",
    "       \n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    \n",
    "    results.iloc[i,0]=  (fp*lfp+fn*lfn)/len(y_val)\n",
    "    results.iloc[i,1]=  1 - accuracy_score(y_val, y_pred)\n",
    "    results.iloc[i,2]=  tp/(tp+fn)\n",
    "    results.iloc[i,3]=  tn/(tn+fp)\n",
    "    results.iloc[i,4]=  precision_score(y_val, y_pred)\n",
    "    results.iloc[i,5]=  average_precision_score(y_val, y_prob)\n",
    "\n",
    "results.iloc[:,0] /= results.iat[0,0]\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Code\n",
    "\n",
    "The two cells below format the notebook for display online. Please omit them from your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css?family=Source+Sans+Pro|Open+Sans:800&display=swap');\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css?family=Source+Sans+Pro|Open+Sans:800&display=swap');\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'css\\\\jupyter.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-88e8ad26c3cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'css\\jupyter.css'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<style>'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstyle\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'</style>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'css\\\\jupyter.css'"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "style = open('css\\jupyter.css', \"r\").read()\n",
    "HTML('<style>'+ style +'</style>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
